{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем датасет tic-tac-toe, т.е. крестики-нолики.\n",
    "\n",
    "В качестве датасета был выбран tic-tac-toe (крестики-нолики). В датасете есть 9 фичей, на каждую клетку игрового поля. \n",
    "\n",
    "Каждая фича принимает значения \"x\", \"o\" - крестик/нолик в поле, либо \"b\" - отсутствие крестика и нолика. Таргет - победили ли крестики.\n",
    "\n",
    "В датасете 958 примеров игр: положительных - 626, отрицательных - 332.\n",
    "\n",
    "\n",
    "В качестве метрик используем Accuracy, Precision, Recall и несколько других, основная - Accuracy.\n",
    "\n",
    "Для оценки качества разбиваем датасет на 10 фолдов, учимся на всех кроме одного, на нем тестируемся. Итоговое качество метода - усреднение по 10 разбиениям.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def read_dataset(filepath):\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data = line.strip().split(',')\n",
    "            y.append(data[-1] == 'positive')\n",
    "            data = data[:-1]\n",
    "            x = set()\n",
    "            for i, element in enumerate(data):\n",
    "                x.add(\"{}_{}\".format(i, element))\n",
    "            X.append(x)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "def get_metrics(y_test, y_score):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_test, y_score),\n",
    "        \"f1_score\": f1_score(y_test, y_score),\n",
    "        \"precision\": precision_score(y_test, y_score),\n",
    "        \"recall\": recall_score(y_test, y_score)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = read_dataset(\"tic.tac-toe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_encode(X):\n",
    "    rows = []\n",
    "    for r in X:\n",
    "        row = []\n",
    "        for element in r:\n",
    "            if 'x' in element:\n",
    "                val = [0, 0, 1]\n",
    "            elif 'o' in element:\n",
    "                val = [0, 1, 0]\n",
    "            else:\n",
    "                val = [1, 0, 0]\n",
    "            row += val\n",
    "        rows.append(row)\n",
    "    return np.array(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изучим, как работает **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_metrics_over_kf(model, kf, X, y,**params):\n",
    "    def get_model(model, x_fit, y_fit, **params):\n",
    "        return model.fit(x_fit, y_fit,**params)\n",
    "    metrics = [get_metrics(get_model(model, X[x_train], y[x_train], **params).predict(X[x_test]), y[x_test]) \n",
    "               for x_train, x_test in kf.split(X_np, y_np)]\n",
    "    keys = metrics[0].keys()\n",
    "    return {k: np.mean([v[k] for v in metrics]) for k in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = dummy_encode(X)\n",
    "y_np = np.array(y).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9548026315789475,\n",
       " 'f1_score': 0.6978891382032744,\n",
       " 'precision': 0.6958333333333334,\n",
       " 'recall': 0.7}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_metrics_over_kf(lr, kf, X_np, y_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression:\n",
    "\n",
    "1. **accuracy**: 0.9548026315789475\n",
    "\n",
    "2. **f1_score**: 0.6978891382032744\n",
    "\n",
    "3. **precision**: 0.6958333333333334\n",
    "\n",
    "4. **recall**: 0.7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем Catboost с категориальными фичами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.649671052631579,\n",
       " 'f1_score': 0.6054329218465744,\n",
       " 'precision': 0.5720416666666667,\n",
       " 'recall': 0.6623376623376623}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = CatBoostClassifier(iterations=100, silent=True)\n",
    "predict_metrics_over_kf(cat, kf, X_np, y_np, cat_features=np.arange(27))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catboost:\n",
    "\n",
    "1. **accuracy**: 0.649671052631579\n",
    "\n",
    "2. **f1_score**: 0.6054329218465744\n",
    "\n",
    "3. **precision**: 0.5720416666666667\n",
    "\n",
    "4. **recall**: 0.6623376623376623\n",
    "\n",
    "\n",
    "Работает ужасно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Приступим к FCA модели.**\n",
    "\n",
    "После долгих попыток перебора моделей, лучшая модель получилась такая:\n",
    "Для каждой тестовой записи считаем supp = |(g+ /\\ g-)+|, и берем две статистики:\n",
    "\n",
    "* min\n",
    "* median\n",
    "\n",
    "Итоговая функция принятия решений:\n",
    "\n",
    "* min(supp) * median(supp) > 9.27e-05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_support(elements, features):\n",
    "    result = []\n",
    "    for element in elements:\n",
    "        intersection = (element & features)\n",
    "        result.append(np.mean(\n",
    "            [(intersection < other_element) for other_element in elements]))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявим модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup = [feats(calculate_support(positive, x))  for x in X[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feats(sup):\n",
    "    f = np.min\n",
    "    g = np.median\n",
    "    value = f(sup) * g(sup)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(X_train, y_train, X_test):\n",
    "    positive = [x for (x, y) in zip(X_train, y_train) if y]\n",
    "    features = np.array([feats(calculate_support(positive, x)) for x in X_test])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9655, 'f1_score': 0.9756, 'precision':  0.9524, 'recall':  1.0000}\n"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train = [X[ind] for ind in train_index]\n",
    "    y_train = [y[ind] for ind in train_index]\n",
    "    X_test = [X[ind] for ind in test_index]\n",
    "    y_test = [y[ind] for ind in test_index](X_train, y_train, X_test)\n",
    "    threshold = 9.27e-05\n",
    "    y_score = features > 9.27e-05\n",
    "    metrics.append(get_metrics(y_score, y_test))\n",
    "\n",
    "keys = metrics[0].keys()\n",
    "\n",
    "print({k: np.mean([v[k] for v in metrics]) for k in keys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FCA: min(supp) * median(supp) > 9.27e-05\n",
    "\n",
    "1. **accuracy**: 0.9655\n",
    "\n",
    "2. **f1_score**: 0.9756\n",
    "\n",
    "3. **precision**: 0.9524\n",
    "\n",
    "4. **recall**: 1.0000\n",
    "\n",
    "\n",
    "Работает хорошо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
